{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import datetime as dt\n",
    "\n",
    "print('Loading Properties ...')\n",
    "properties2016 = pd.read_csv('properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('properties_2017.csv', low_memory = False)\n",
    "\n",
    "print('Loading Train ...')\n",
    "train2016 = pd.read_csv('train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "train2017 = pd.read_csv('train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "\n",
    "def add_date_features(df):\n",
    "    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transaction_month\"] = df[\"transactiondate\"].dt.month\n",
    "    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n",
    "    df[\"transaction_quarter\"] = df[\"transactiondate\"].dt.quarter\n",
    "    df.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "train2016 = add_date_features(train2016)\n",
    "train2017 = add_date_features(train2017)\n",
    "\n",
    "print('Loading Sample ...')\n",
    "sample_submission = pd.read_csv('sample_submission.csv', low_memory = False)\n",
    "\n",
    "print('Merge Train with Properties ...')\n",
    "train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n",
    "train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n",
    "\n",
    "#print('Tax Features 2016  ...')\n",
    "#train2016.iloc[:, train2016.columns.str.startswith('tax')] = np.nan\n",
    "\n",
    "print('Concat Train 2016 & 2017 ...')\n",
    "train_df = pd.concat([train2016, train2017], axis = 0)\n",
    "test_df = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')\n",
    "\n",
    "del properties2016, properties2017, train2016, train2017\n",
    "gc.collect();\n",
    "\n",
    "print('Remove missing data fields ...')\n",
    "\n",
    "missing_perc_thresh = 0.98\n",
    "exclude_missing = []\n",
    "num_rows = train_df.shape[0]\n",
    "for c in train_df.columns:\n",
    "    num_missing = train_df[c].isnull().sum()\n",
    "    if num_missing == 0:\n",
    "        continue\n",
    "    missing_frac = num_missing / float(num_rows)\n",
    "    if missing_frac > missing_perc_thresh:\n",
    "        exclude_missing.append(c)\n",
    "print(\"We exclude: %s\" % len(exclude_missing))\n",
    "\n",
    "del num_rows, missing_perc_thresh\n",
    "gc.collect();\n",
    "\n",
    "print (\"Remove features with one unique value !!\")\n",
    "exclude_unique = []\n",
    "for c in train_df.columns:\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if train_df[c].isnull().sum() != 0:\n",
    "        num_uniques -= 1\n",
    "    if num_uniques == 1:\n",
    "        exclude_unique.append(c)\n",
    "print(\"We exclude: %s\" % len(exclude_unique))\n",
    "\n",
    "print (\"Define training features !!\")\n",
    "exclude_other = ['parcelid', 'logerror','propertyzoningdesc']\n",
    "train_features = []\n",
    "for c in train_df.columns:\n",
    "    if c not in exclude_missing \\\n",
    "       and c not in exclude_other and c not in exclude_unique:\n",
    "        train_features.append(c)\n",
    "print(\"We use these for training: %s\" % len(train_features))\n",
    "\n",
    "print (\"Define categorial features !!\")\n",
    "cat_feature_inds = []\n",
    "cat_unique_thresh = 1000\n",
    "for i, c in enumerate(train_features):\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if num_uniques < cat_unique_thresh \\\n",
    "       and not 'sqft' in c \\\n",
    "       and not 'cnt' in c \\\n",
    "       and not 'nbr' in c \\\n",
    "       and not 'number' in c:\n",
    "        cat_feature_inds.append(i)\n",
    "        \n",
    "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n",
    "\n",
    "print (\"Replacing NaN values by -999 !!\")\n",
    "train_df.fillna(-999, inplace=True)\n",
    "test_df.fillna(-999, inplace=True)\n",
    "\n",
    "print (\"Training time !!\")\n",
    "X_train = train_df[train_features]\n",
    "y_train = train_df.logerror\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "test_df['transactiondate'] = pd.Timestamp('2016-12-01') \n",
    "test_df = add_date_features(test_df)\n",
    "X_test = test_df[train_features]\n",
    "print(X_test.shape)\n",
    "\n",
    "test_data = {'201610':'2016-10-15',\n",
    "             '201611':'2016-11-15',\n",
    "             '201612':'2016-12-15',\n",
    "             '201710':'2017-10-15',\n",
    "             '201711':'2017-11-15',\n",
    "             '201712':'2017-12-15'}\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "num_ensembles = 5\n",
    "for i in tqdm(range(num_ensembles)):\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=630, learning_rate=0.03,\n",
    "        depth=6, l2_leaf_reg=3,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=i)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        cat_features=cat_feature_inds)\n",
    "    for label,date in test_data.items():\n",
    "        test_df['transactiondate'] = pd.Timestamp(date) \n",
    "        test_df = add_date_features(test_df)\n",
    "        X_test = test_df[train_features]\n",
    "        submission[label] += model.predict(X_test)\n",
    "submission.iloc[:,1:7] /= num_ensembles\n",
    "del train_df,test_df,X_train,y_train,X_test\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST, OLS and  Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "\n",
    "properties = pd.read_csv('properties_2017.csv')\n",
    "train = pd.read_csv('train_2017.csv')\n",
    "xgb_sub = pd.read_csv('sample_submission.csv')\n",
    "##### PROCESS DATA FOR XGBOOST\n",
    "print( \"\\nProcessing data for XGBoost ...\")\n",
    "for c in properties.columns:\n",
    "    properties[c]=properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "#x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "x_test = pd.merge(xgb_sub[['ParcelId']],properties, how='left', left_on='ParcelId', right_on='parcelid')\n",
    "#x_test.drop('ParcelId',axis=1)\n",
    "# shape        \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "# drop out ouliers\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "x_test = x_test[x_train.columns]\n",
    "\n",
    "print('After removing outliers:')     \n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "num_boost_rounds = 250\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "# train model\n",
    "print( \"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "xgb_pred1 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nFirst XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred1).head() )\n",
    "\n",
    "##### RUN XGBOOST AGAIN\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.033,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "num_boost_rounds = 150\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "print( \"\\nTraining XGBoost again ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost again ...\")\n",
    "xgb_pred2 = model.predict(dtest)\n",
    "\n",
    "print( \"\\nSecond XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred2).head() )\n",
    "\n",
    "##### COMBINE XGBOOST RESULTS\n",
    "\n",
    "xgb_pred = 0.8*xgb_pred1 + 0.2*xgb_pred2\n",
    "#xgb_pred = xgb_pred1\n",
    "\n",
    "print( \"\\nCombined XGBoost predictions:\" )\n",
    "print( pd.DataFrame(xgb_pred).head() )\n",
    "\n",
    "del train_df\n",
    "del x_train\n",
    "del x_test\n",
    "del properties\n",
    "del dtest\n",
    "del dtrain\n",
    "del xgb_pred1\n",
    "del xgb_pred2 \n",
    "gc.collect()\n",
    "\n",
    "##### OLS\n",
    "np.random.seed(17)\n",
    "random.seed(17)\n",
    "\n",
    "train = pd.read_csv(\"train_2017.csv\", parse_dates=[\"transactiondate\"])\n",
    "properties = pd.read_csv(\"properties_2017.csv\")\n",
    "xgb_ols = pd.read_csv(\"sample_submission.csv\")\n",
    "print(len(train),len(properties))\n",
    "\n",
    "def get_features(df):\n",
    "    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n",
    "    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n",
    "    df['transactiondate'] = df['transactiondate'].dt.quarter\n",
    "    df = df.fillna(-1.0)\n",
    "    return df\n",
    "\n",
    "train = pd.merge(train, properties, how='left', on='parcelid')\n",
    "y = train['logerror'].values\n",
    "test = pd.merge(xgb_ols, properties, how='left', left_on='ParcelId', right_on='parcelid')\n",
    "properties = [] #memory\n",
    "\n",
    "exc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\n",
    "col = [c for c in train.columns if c not in exc]\n",
    "\n",
    "train = get_features(train[col])\n",
    "test['transactiondate'] = '2017-10-01' #should use the most common training date\n",
    "test = get_features(test[col])\n",
    "\n",
    "reg = LinearRegression(n_jobs=-1)\n",
    "reg.fit(train, y); print('fit...')\n",
    "\n",
    "test_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\n",
    "test_columns = ['201610','201611','201612','201710','201711','201712']\n",
    "\n",
    "##### Combine xgb, mean and OLS\n",
    "pred0 = (1-0.006)*xgb_pred + 0.006*np.mean(y)\n",
    "for i in range(len(test_dates)):\n",
    "    test['transactiondate'] = test_dates[i]\n",
    "    pred = 0.08*reg.predict(get_features(test)) + (1-0.08)*pred0\n",
    "    xgb_ols[test_columns[i]] = [float(format(x, '.6f')) for x in pred]\n",
    "    print('predict...', i)\n",
    "    \n",
    "del train,properties,test,y,pred0,pred,xgb_pred\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ParcelId</th>\n",
       "      <th>201610</th>\n",
       "      <th>201611</th>\n",
       "      <th>201612</th>\n",
       "      <th>201710</th>\n",
       "      <th>201711</th>\n",
       "      <th>201712</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.062555</td>\n",
       "      <td>0.062579</td>\n",
       "      <td>0.062604</td>\n",
       "      <td>0.062555</td>\n",
       "      <td>0.062579</td>\n",
       "      <td>0.062604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.037543</td>\n",
       "      <td>0.037568</td>\n",
       "      <td>0.037593</td>\n",
       "      <td>0.037543</td>\n",
       "      <td>0.037568</td>\n",
       "      <td>0.037593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>-0.065061</td>\n",
       "      <td>-0.065037</td>\n",
       "      <td>-0.065012</td>\n",
       "      <td>-0.065061</td>\n",
       "      <td>-0.065037</td>\n",
       "      <td>-0.065012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>-0.010865</td>\n",
       "      <td>-0.010840</td>\n",
       "      <td>-0.010815</td>\n",
       "      <td>-0.010865</td>\n",
       "      <td>-0.010840</td>\n",
       "      <td>-0.010815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.005928</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.005928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ParcelId    201610    201611    201612    201710    201711    201712\n",
       "0  10754147  0.062555  0.062579  0.062604  0.062555  0.062579  0.062604\n",
       "1  10759547  0.037543  0.037568  0.037593  0.037543  0.037568  0.037593\n",
       "2  10843547 -0.065061 -0.065037 -0.065012 -0.065061 -0.065037 -0.065012\n",
       "3  10859147 -0.010865 -0.010840 -0.010815 -0.010865 -0.010840 -0.010815\n",
       "4  10879947  0.005878  0.005903  0.005928  0.005878  0.005903  0.005928"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_ols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print('Loading data...')\n",
    "properties2016 = pd.read_csv('properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('properties_2017.csv', low_memory = False)\n",
    "train2016 = pd.read_csv('train_2016_v2.csv')\n",
    "train2017 = pd.read_csv('train_2017.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv', low_memory = False)\n",
    "train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n",
    "train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n",
    "train = pd.concat([train2016, train2017], axis = 0)\n",
    "test = pd.merge(sample_submission[['ParcelId']], properties2017.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')\n",
    "del properties2016, properties2017, train2016, train2017\n",
    "gc.collect();\n",
    "\n",
    "\n",
    "print('Memory usage reduction...')\n",
    "train[['latitude', 'longitude']] /= 1e6\n",
    "test[['latitude', 'longitude']] /= 1e6\n",
    "\n",
    "train['censustractandblock'] /= 1e12\n",
    "test['censustractandblock'] /= 1e12\n",
    "\n",
    "for column in test.columns:\n",
    "    if test[column].dtype == int:\n",
    "        test[column] = test[column].astype(np.int32)\n",
    "    if test[column].dtype == float:\n",
    "        test[column] = test[column].astype(np.float32)\n",
    "      \n",
    "        \n",
    "print('Feature engineering...')\n",
    "#train['year'] = pd.to_datetime(train['transactiondate']).dt.year\n",
    "train['month'] = pd.to_datetime(train['transactiondate']).dt.month\n",
    "train = train.drop('transactiondate', axis = 1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "non_number_columns = train.dtypes[train.dtypes == object].index.values\n",
    "\n",
    "for column in non_number_columns:\n",
    "    train_test = pd.concat([train[column], test[column]], axis = 0)\n",
    "    encoder = LabelEncoder().fit(train_test.astype(str))\n",
    "    train[column] = encoder.transform(train[column].astype(str)).astype(np.int32)\n",
    "    test[column] = encoder.transform(test[column].astype(str)).astype(np.int32)\n",
    "    \n",
    "feature_names = [feature for feature in train.columns[2:] if feature != 'month']\n",
    "\n",
    "month_avgs = train.groupby('month').agg('mean')['logerror'].values - train['logerror'].mean()\n",
    "                             \n",
    "print('Preparing arrays and throwing out outliers...')\n",
    "X_train = train[feature_names].values\n",
    "y_train = train['logerror'].values\n",
    "X_test = test[feature_names].values\n",
    "\n",
    "del test\n",
    "gc.collect();\n",
    "\n",
    "month_values = train['month'].values\n",
    "month_avg_values = np.array([month_avgs[month - 1] for month in month_values]).reshape(-1, 1)\n",
    "X_train = np.hstack([X_train, month_avg_values])\n",
    "\n",
    "X_train = X_train[np.abs(y_train) < 0.4, :]\n",
    "y_train = y_train[np.abs(y_train) < 0.4]\n",
    "\n",
    "kfolds = 4\n",
    "\n",
    "models = []\n",
    "kfold = KFold(n_splits = kfolds, shuffle = True)\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    \n",
    "    print('Training LGBM model with fold {}...'.format(i + 1))\n",
    "    X_train_, y_train_ = X_train[train_index], y_train[train_index]\n",
    "    X_valid_, y_valid_ = X_train[test_index], y_train[test_index]\n",
    "    \n",
    "    ltrain = lgb.Dataset(X_train_, label = y_train_, free_raw_data = False)\n",
    "    lvalid = lgb.Dataset(X_valid_, label = y_valid_, free_raw_data = False)\n",
    "    \n",
    "    params = {}\n",
    "    params['metric'] = 'mae'\n",
    "    params['max_depth'] = 100\n",
    "    params['num_leaves'] = 32\n",
    "    params['feature_fraction'] = .85\n",
    "    params['bagging_fraction'] = .95\n",
    "    params['bagging_freq'] = 8\n",
    "    params['learning_rate'] = 0.01\n",
    "    params['verbosity'] = 0\n",
    "    \n",
    "    models.append(lgb.train(params, ltrain, valid_sets = [ltrain, lvalid], \n",
    "            verbose_eval=200, num_boost_round=1000))\n",
    "                  \n",
    "                  \n",
    "print('Making predictions and praying for good results...')\n",
    "X_test = np.hstack([X_test, np.zeros((X_test.shape[0], 1))])\n",
    "folds = 10\n",
    "n = int(X_test.shape[0] / folds)\n",
    "\n",
    "for j in tqdm(range(folds)):\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    if j < folds - 1:\n",
    "            X_test_ = X_test[j*n: (j+1)*n, :]\n",
    "            results['ParcelId'] = sample_submission['ParcelId'].iloc[j*n: (j+1)*n]\n",
    "    else:\n",
    "            X_test_ = X_test[j*n: , :]\n",
    "            results['ParcelId'] = sample_submission['ParcelId'].iloc[j*n: ]\n",
    "            \n",
    "    for month in [10, 11, 12]:\n",
    "        X_test_[:, -1] = month_avgs[month - 1]\n",
    "        assert X_test_.shape[1] == X_test.shape[1]\n",
    "        y_pred = np.zeros(X_test_.shape[0])\n",
    "        for model in models:\n",
    "            y_pred += model.predict(X_test_) / kfolds\n",
    "        results['2016'+ str(month)] = y_pred\n",
    "        results['2017'+ str(month)] = y_pred\n",
    "    \n",
    "    if j == 0:\n",
    "        results_ = results.copy()\n",
    "    else:\n",
    "        results_ = pd.concat([results_, results], axis = 0)\n",
    "    \n",
    "    \n",
    "print('Saving predictions...')\n",
    "lgb_results = results_[sample_submission.columns]\n",
    "assert lgb_results.shape == sample_submission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine results which computed by above methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_methods = lgb_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_methods.iloc[:,1:7] = 0.6*submission + 0.3*lgb_results.iloc[:,1:7] + \\\n",
    "                            0.1*xgb_ols.iloc[:,1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ParcelId</th>\n",
       "      <th>201610</th>\n",
       "      <th>201611</th>\n",
       "      <th>201612</th>\n",
       "      <th>201710</th>\n",
       "      <th>201711</th>\n",
       "      <th>201712</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10754147</td>\n",
       "      <td>0.019003</td>\n",
       "      <td>0.019449</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.019080</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.019278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10759547</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>0.008339</td>\n",
       "      <td>0.009420</td>\n",
       "      <td>0.008578</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.009674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10843547</td>\n",
       "      <td>0.016101</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.017389</td>\n",
       "      <td>0.015731</td>\n",
       "      <td>0.014207</td>\n",
       "      <td>0.016035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10859147</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>0.023040</td>\n",
       "      <td>0.022669</td>\n",
       "      <td>0.023199</td>\n",
       "      <td>0.021748</td>\n",
       "      <td>0.021420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10879947</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.003104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ParcelId    201610    201611    201612    201710    201711    201712\n",
       "0  10754147  0.019003  0.019449  0.020227  0.019080  0.018554  0.019278\n",
       "1  10759547  0.007650  0.008339  0.009420  0.008578  0.008290  0.009674\n",
       "2  10843547  0.016101  0.015559  0.017389  0.015731  0.014207  0.016035\n",
       "3  10859147  0.023825  0.023040  0.022669  0.023199  0.021748  0.021420\n",
       "4  10879947  0.000136  0.003164  0.002299  0.002035  0.003977  0.003104"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_methods.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combine_methods.to_csv('final_2017.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
